---
title: "PML Data Prediction"
author: "Sarat Konduru"
date: "Thursday, January 22, 2015"
output:
  html_document:
    keep_md: yes
    toc: yes
---

## Synopsis

This data analysis report is created as part of the project work done in association with the 'Practical Machine Learning' class in the Coursera's Data Science track.

The data for this project come from this source: <http://groupware.les.inf.puc-rio.br/har>.

### HAR Dataset for benchmarking

Important: you are free to use this dataset for any purpose. This dataset is licensed under the Creative Commons license (CC BY-SA). The CC BY-SA license means you can remix, tweak, and build upon this work even for commercial purposes, as long as you credit the authors of the original work and you license your new creations under the identical terms we are licensing to you. This license is often compared to "copyleft" free and open source software licenses. All new works based on this dataset will carry the same license, so any derivatives will also allow commercial use.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3PjaRxXP4

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
Cited by 2 (Google Scholar)

Read more: <http://groupware.les.inf.puc-rio.br/har#ixzz3PjaB1VDY>


The goal of this data analysis is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.


## Dataset

The training data for this project are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>.

The test data are available here: <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>.

This analysis assumes both the training and testing data sets are downloaded and are placed in the project working directory. 

```{r , cache=FALSE, echo=FALSE}
require(caret)
setwd('C:/Coursera/Data Science - Specialization/Practical Machine Learning/project')
source('functions.R')
```

```{r DATA_LOADING, cache=TRUE}

#Read the training and the testing sets
train = loadDataSet("pml-training.csv")
finaltest = loadDataSet("pml-testing.csv")

# Dump the variables in the data set to get an idea of what variables we are dealing with
names(train)
```

## Preprocessing

### Basic manual pre-processing
Studying the variables in the data set, we do not need all the variables for this analysis. Some columns are aggregate columns and have mostly NA's. There are time window variables which we can get rid of, since it should not have an impact on the outcome. Also there are redundant timestamp variables. We can get rid of the individual timestamp part1 and part2.

```{r PRE_PROCESS, cache=FALSE}

# Remove the unimportant variables from the dataset
colsToRemove <- c(1,2,3,4,5,6,7)

#  Perform prelimenary manual preProcessing  
train = transform(train, colsToRemove)
finaltest = transform(finaltest, colsToRemove)
```

### Sub-sampling for model validation
Split the training data set into two subsets for validating our data model

```{r DATA_SPLIT, cache=TRUE}
set.seed(32415)

inTrain = createDataPartition(y=train$classe, p=0.7, list=FALSE)

subTrain = train[inTrain,]
subTest = train[-inTrain,]
```

### Check for near zero variance variables
```{r ZERO_VAR, cache=TRUE}
outcome = which(names(subTrain) %in% "classe")

# Find out if there exists and near Zero variance variables
noVariance = nearZeroVar(subTrain[,-outcome])

# If there are, remove them from all the data sets
if(length(noVariance) > 0){
  subTrain = subTrain[,-noVariance]
  subTest = subTest[,-noVariance]
  finaltest = finaltest[,-noVariance]
}
```

We found `r length(noVariance)` variables having near zero variance in the data set. 

### Check if there are any more variables that have near zero variance
```{r HIGH_COR, cache=TRUE}
outcome = which(names(subTrain) %in% "classe")

# Calculate the correlation matrix for the subTrain dataset
cr = cor(subTrain[,-outcome])

# Filter out variables having a high correlation
highlyCorVars <- findCorrelation(cr, cutoff = .75)

if( length(highlyCorVars) > 0 ){
  subTrain = subTrain[,-highlyCorVars]
  subTest = subTest[,-highlyCorVars]
  finaltest = finaltest[,-highlyCorVars]
}

#summary(subTrain)

```

We found `r length(highlyCorVars)` variables having hight correlation in the data set. 

We are now left with `r length(names(subTrain))-1` variables left exluding the outcome. 

### Visualizations

We shall examine the rest of the variables using pairs' plots. Since there are a lot of observations (`r nrow(subTrain)`), we will create a further sub-sample just for the purposes of plotting.

```{r PLOTS, cache=FALSE, fig.width=12, fig.height=12}

forPlots = createDataPartition(y=subTrain$classe, p=0.1, list=FALSE)
subPlots = subTrain[forPlots,]

featurePlot(x=(log10(abs(subPlots[,c(1:7)])+1)), y=subPlots$classe, plot="pairs")
featurePlot(x=(log10(abs(subPlots[,c(8:14)])+1)), y=subPlots$classe, plot="pairs")
featurePlot(x=(log10(abs(subPlots[,c(15:21)])+1)), y=subPlots$classe, plot="pairs")
featurePlot(x=(log10(abs(subPlots[,c(22:28)])+1)), y=subPlots$classe, plot="pairs")
featurePlot(x=(log10(abs(subPlots[,c(29:34)])+1)), y=subPlots$classe, plot="pairs")
```

From looking at the plots, there seems to be some variables that are still corealted. 

### Principal Component Analysis

We can perform a pricipal component analysis to see if the data space can be further reduced.
```{r PCA, cache=TRUE}
outcome = which(names(subTrain) %in% "classe")

PCAProc = preProcess(subTrain[,-outcome], method= c("pca"))

trainPCs = predict(PCAProc,     newdata = subTrain[,-outcome])
testPCs = predict(PCAProc,      newdata = subTest[,-outcome])
finaltestPCs = predict(PCAProc, newdata = finaltest[,-outcome])

PCAProc
```

## Model fitting

Now that we have preprocessed the data set and reduced the variables, we can train a model using the PCAs generated above. We will fit a random forest model.
```{r MODEL_FIT, cache=TRUE}
fitControl = trainControl(method = "oob", number = 4, verboseIter = TRUE)
useMethod = "rf";
m = train(subTrain$classe ~ ., method=useMethod, trControl= fitControl, data=trainPCs, verbose=TRUE)
```

### Model error rates
The In-Sample error for the model is: 
```{r IN_SAMPLE_ERR, cache=TRUE}
confusionMatrix(subTrain$classe, predict(m, newdata = trainPCs))
```

The Out-Sample error for the model is:
```{r OUT_SAMPLE_ERR, cache=TRUE}
confusionMatrix(subTest$classe, predict(m, newdata = testPCs))
```

## Prediction
Now finally using the model, predict the final 20 test set and write into the text files that are to be submitted.
```{r Prediction, cache=TRUE}
answers = as.character(predict(m, newdata = finaltestPCs))

pml_write_files(answers, useMethod)
```

## Summary
The generated files were submitted to the coursera website.